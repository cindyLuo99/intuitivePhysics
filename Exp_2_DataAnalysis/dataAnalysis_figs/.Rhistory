cex.names = 0.8)
plot(Goals ~ Assists, data = hockey_stats, main = 'Goals as a function of Assists',
xlab = 'Goals', ylab = 'Assists',  pch = 4, col = c('lightblue', 'purple', 'lightgreen', 'orange')[hockey_stats$Position])
legend(0, 70, legend = levels(hockey_stats$Position), cex = .9, pch = 4, col = c('lightblue', 'purple', 'lightgreen', 'orange'))
hist(hockey_stats$Goals)
# Let's start with Prediction
# Fit the model. The dependent variable is Goals (which is count data) so I'm using a negative binomial
# with position and assists as predictors.
nb_goals_assists <- glmmTMB(Goals ~ Position + Assists,
data = hockey_stats, family = "nbinom2")
# Quick Assumptions Checks
set.seed(123)
simres <- simulateResiduals(fittedModel = nb_goals_assists, n = 250)
hist(simres$scaledResiduals, xlab = "scaled residuals", main = "Residual Histogram")  ## This looks better
plotQQunif(simres, testUniformity = FALSE, testOutliers = FALSE, testDispersion = FALSE)    ## This looks okay.
#Generate predicted probabilities for response.
predict(nb_goals_assists)                         ## logit scale predictions
pred_goals_assists_df <- data.frame(Goals = hockey_stats$Goals, Goals_hat = predict(nb_goals_assists))        ## merge y_i and \hat y_i into a data frame
head(pred_goals_assists_df)
tail(pred_goals_assists_df)     ## probability scale predictions
## Predicting new observations: Assume that three new players come in and we have
## the following predictor values. Response values are NOT observed.
goals_new <- data.frame(Position = c("Center", "Defense", "Left Wing", "Right Wing"),
Assists = c(5, 12, 20, 41)
)
goals_new
colnames(goals_new)
predict(nb_goals_assists, newdata = goals_new)
# These are the predicted number of Goals for these four new players.So for the player in
# Center position who has 5 assists, the predicted number of goals in 1.46. For the player in Defense
# position with 12 assists, the predicted number of goals is 0.89. For the player in the Left Wing position,
# with 20 assists, the predicted number of goals in 2.26. For the player in the Right Wing position with
# 41 assists, the predicted number of goals is 3.54.
head(pred_goals_assists_df, 20)                ## data frame with y_i and \hat y_i
MSE_cc <- mean((pred_goals_assists_df[,1] - pred_goals_assists_df[,2])^2)    ## cf. formula on the slides
MSE_cc
RMSE_cc <- sqrt(MSE_cc)             ## RMSE
RMSE_cc
# Interpretation: On average, our predictions deviate from the actual number of Goals by approximately 12.45 points.
# which is not great.
# Now, let's practice handling missing data.
set.seed(123)
hockey_stats2 <- mice::ampute(hockey_stats, mech = "MAR")$amp
hockey_stats2$Goals <- hockey_stats$Goals
hockey_stats2$Position <- hockey_stats$Position
hockey_stats2$Player <- hockey_stats$Player
str(hockey_stats2)
#  Visualize the missing data pattern.
vis_miss(hockey_stats2)
vis_dat(hockey_stats2)
#  Normally, we'd diagnose the missingness mechanism visually, but we
#  know that this is a MAR situation.
# Perform multiple imputation.
# Creating 5 complete datasets by imputing missing values
imp_hockey_stats2 <- mice(hockey_stats2, m = 15, maxit = 5, seed = 123)
summary(imp_hockey_stats2)
head(imp_hockey_stats2)
# Look at imputed datasets.
mice::complete(imp_hockey_stats2, 1) %>% head()
mice::complete(imp_hockey_stats2, 3) %>% head()
mice::complete(imp_hockey_stats2, 5) %>% head()
plot(imp_hockey_stats2)         ## convergence check- seems okay
lattice::densityplot(imp_hockey_stats2)
## Fitting a model on the imputed datasets.
hockeystats_mi <- with(imp_hockey_stats2, glm(Goals ~ Position + Assists + PenaltyMinutes + Country, family = "poisson"))
hockeystats_mi
## Pooling the results and summarizing the output.
hockeystats2_pool <- mice::pool(hockeystats_mi)
summary(hockeystats_mi)
# Now, let's compare the imputed output of model to the original output of the model:
hockey.glm <- glm(Goals ~ Position + Assists + PenaltyMinutes + Country, data = hockey_stats, family = "poisson")
summary(hockey.glm)
confint.default(hockey.glm)
# When we compare the model on the imputed data and the model on the original data, we can see that the
# outputs look very similar, and there are no major differences.
# Now, let's try using a lasso on this dataset.
## Determine lambda through CV
set.seed(123)
hockey_glm_lasso <- cv.glmnet(x = as.matrix(hockey_stats[,-6]), y = hockey_stats$Goals, type.measure = "mse", nfolds = 10)
# The sixth column in the dataframe is Goals.
hockey_glm_lasso$lambda
hockey_glm_lasso$lambda.min       ## optimal lambda parameter
hockey_glm_lasso$lambda.1se       ## largest value of lambda
log(hockey_glm_lasso$lambda.min)       ## optimal lambda parameter (log-scale)
log(hockey_glm_lasso$lambda.1se)       ## largest lambda (log-scale) within 1 standard error
plot(hockey_glm_lasso)
## Regularization plot
hockey_glm_lasso$lambda
glmnet(x = hockey_stats[,-6], y = hockey_stats$Goals, lambda = hockey_glm_lasso$lambda) |> plot(xvar = "lambda", label = TRUE)
rbind(colnames(hockey_stats[,-6]), 1:ncol(hockey_stats[,-6]))
# Fit models
# Fit the lasso model with the optimal lambda value
hockey_glm_lasso$lambda.min
hockey_glm_lasso1 <- glmnet(x = hockey_stats[,-6], y = hockey_stats$Goals, lambda = hockey_glm_lasso$lambda.min)
hockey_glm_lasso1$beta
## In case we need want shrinkage, let's take the larger value:
hockey_glm_lasso$lambda.1se
hockey_glm_lasso2 <- glmnet(x = hockey_stats[,-6], y = hockey_stats$Goals, lambda = hockey_glm_lasso$lambda.1se)
hockey_glm_lasso2$beta
# We can see from this output that the predictors with the strongest positive effects are Points, Game Winning
# Goals and Power Play Goals. Six predictors were removed (team, country, position,
# Games Played, Plus Minus, and Penalty Minutes).
##------------------------------------------------------------------------------------
##------------------------------------------------------------------------------------
# Below is completed by: Cindy Luo
## Purpose of this analysis:
## We are interested in how people infer a physical scene. Specifically, when
## shown a potential answer for a proposed obstacle position (based on different biases)
## under different time constraints, how likely are participants to accept that
## proposed position as the correct one?
##
## DV: response [accept ("j" keyboard response) or reject ("f" keyboard response)]
## IVs:
##    - dist_from_gt: Euclidean distance of the proposed position from the ground truth (metric)
##    - choice: probe type (the type of bias associated with the proposed position, categorical)
##    - timeLimit: time limit for that trial (categorical)
# ============================================================================
# Section 0: Setup & Package Loading
# ============================================================================
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
require(pacman)
p_load(visdat, vtree, tidyverse, lme4, lmerTest, glmmTMB, performance,
GGally, datasets, sjPlot, mgcv, ggstatsplot, caret, pROC, ggeffects, ggplot2, mgcViz, gratia, itsadug, gamm4)
# ============================================================================
# Section 1: Data Preparation & Cleaning
# ============================================================================
# Load the dataset
data_full <- read_csv("df_all_pilot_v2_19participants.csv")
cat("Dimensions of full dataset: ", dim(data_full), "\n")
head(data_full)
str(data_full)
# Recode responses and compute derived distance variable
data_full$response_numeric <- ifelse(data_full$response == "j", 1, 0) # recode j/f responses to 1/0
data_full$dist_from_gt <- sqrt(
(data_full$obstacle_choice_x - data_full$obstacle_groundTruth_x)^2 +
(data_full$obstacle_choice_y - data_full$obstacle_groundTruth_y)^2
)
# Check if there are any missing values
vis_miss(data_full) # missing data in response & rt
# Only keep the complete cases in the analysis
data_full <- data_full[!is.na(data_full$response), ]
# Subset rows where choice is either "mid" or "lp" (will only be focusing on response to these two probe types)
df_mid_lp <- data_full[data_full$choice %in% c("mid", "lp"), ]
# Check the levels of categorical variables
df_mid_lp$choice <- factor(df_mid_lp$choice, levels = c("lp", "mid")) # Convert 'choice' to a factor with levels ordered as "lp" then "mid"
df_mid_lp$timeLimit <- factor(df_mid_lp$timeLimit)
df_mid_lp$subject_id <- factor(df_mid_lp$subject_id)
# Check participant counts and variable tree structure
table(df_mid_lp$subject_id) # max 96 measurements per participant
n_distinct(df_mid_lp$subject_id) # 19 participants
vtree(df_mid_lp, vars = ~ stimulus_idx + choice + timeLimit)
# variable tree: each participant got tested 12 stimulus, each stimuli is associated with 2 types of choice (lp, mid),
# each choice has 2 trials with different time limits (also, each trial repeats 2 times)
# ============================================================================
# Section 2: Exploratory Data Analysis (EDA)
# ============================================================================
vis_miss(df_mid_lp)  # No missing data now
# histogram
hist(df_mid_lp$dist_from_gt, main = "Dist from GT", xlab = "Dist from GT")
# Bar plot for response (numeric coded)
ggplot(df_mid_lp, aes(x = factor(response_numeric))) +
geom_bar() +
labs(title = "Frequency of Response (1 = Accept)", x = "Response", y = "Count")
# Scatterplot: Distance vs. Response
ggplot(df_mid_lp, aes(x = dist_from_gt, y = response_numeric, color = choice)) +
geom_jitter(width = 0.5, height = 0.1, alpha = 0.6) +
labs(title = "Scatterplot: Distance vs. Response",
x = "Distance from Ground Truth", y = "Response (1 = Accept)", color = "Choice",
fill = "Choice")
# Aggregated proportion of acceptance across participants (by condition)
df_summary <- df_mid_lp %>%
group_by(subject_id, choice, timeLimit) %>%
summarise(prop_j = mean(response_numeric), .groups = "drop")
ggplot(df_summary, aes(x = choice, y = prop_j, fill = timeLimit)) +
geom_boxplot(position = position_dodge(width = 0.8), alpha = 0.7, outlier.shape = NA) +
geom_jitter(position = position_jitterdodge(dodge.width = 0.8, jitter.width = 0.2), alpha = 0.5) +
labs(title = "Proportion of 'j' Responses by Probe Type and Time Limit",
x = "Choice", y = "Proportion of 'j' Responses", fill = "Time Limit (ms)") +
theme_minimal(base_size = 14)
# ============================================================================
# Section 3: Model Fitting - Candidate GAMs
# ============================================================================
# All models include a random intercept for subject.
# Model 1: Full interaction (parametric): dist_from_gt + choice * timeLimit + random intercept per subject
model1 <- gam(response_numeric ~ dist_from_gt + choice * timeLimit + s(subject_id, bs = "re"),
data = df_mid_lp, family = binomial(link = "logit"))
# Model 2: Main effects: dist_from_gt + choice + timeLimit + random intercept per subject
model2 <- gam(response_numeric ~ dist_from_gt + choice + timeLimit + s(subject_id, bs = "re"),
data = df_mid_lp, family = binomial(link = "logit"))
# Model 3: No choice effect: dist_from_gt + timeLimit + random intercept per subject
model3 <- gam(response_numeric ~ dist_from_gt + timeLimit + s(subject_id, bs = "re"),
data = df_mid_lp, family = binomial(link = "logit"))
# Model 4: No timeLimit effect: dist_from_gt + choice + random intercept per subject
model4 <- gam(response_numeric ~ dist_from_gt + choice + s(subject_id, bs = "re"),
data = df_mid_lp, family = binomial(link = "logit"))
# Model 5: Distance only: dist_from_gt + random intercept per subject
model5 <- gam(response_numeric ~ dist_from_gt + s(subject_id, bs = "re"),
data = df_mid_lp, family = binomial(link = "logit"))
# Model 6: Model 4 with a smooth function on dist_from_gt
model6 <- gam(response_numeric ~ s(dist_from_gt) + choice * timeLimit + s(subject_id, bs = "re"),
data = df_mid_lp, family = binomial(link = "logit"))
# Model 7: Model 6 with a different smooth for dist_from_gt for each probe type
model7 <- gam(response_numeric ~ s(dist_from_gt, by = choice) + s(subject_id, bs = "re"),
data = df_mid_lp, family = binomial(link = "logit"))
# Model 8: Model 7 with an additional interaction effect between choice and timeLimit
model8 <- gam(response_numeric ~ s(dist_from_gt, by = choice) + choice * timeLimit + s(subject_id, bs = "re"),
data = df_mid_lp, family = binomial(link = "logit"))
# ============================================================================
# Section 4: Model Comparison & Selection
# ============================================================================
# Compare models using AIC and BIC and R^2
AIC(model1, model2, model3, model4, model5 , model6, model7, model8) # model 8 has the lowest AIC
BIC(model1, model2, model3, model4, model5, model6, model7) # model 4 has the lowest BIC
models <- list(m1 = model1, m2 = model2, m3 = model3, m4 = model4,
m5 = model5, m6 = model6, m7 = model7, m8 = model8)
r2_values <- sapply(models, function(mod) summary(mod)$r.sq)
r2_values # model 8 has the highest R^2
anova(model1, model2, model3, model4, model5, model6, model7, model8, test = "LRT")  # Model 8 is best model among the models
anova(model1, model6, model7, model8)
# Model 8 is the best model
summary(model8)
best_model <- gam(response_numeric ~ s(dist_from_gt, by = choice) + choice * timeLimit + s(subject_id, bs = "re"),
data = df_mid_lp, family = binomial(link = "logit"), method = "REML")     ## re-fit with REML
# ============================================================================
# Section 5: Model Predictions & Diagnostic Plots
# ============================================================================
## Residual check and PPC
appraise(best_model)                                         ## not very good
set.seed(42)
vgam_best <- getViz(best_model, nsim = 100, post = TRUE, unconditional = TRUE)   ## simulate 100 response vectors
check0D(vgam_best, type = "y") + l_dens1D()                 ## some discrepancies
# Plot predicted probabilities by probe type and time limit
plot_model(best_model, type = "pred", terms = c("dist_from_gt", "choice", "timeLimit"))
# Note: There is a huge error range for mid probe types beyond a distance of 130
# Overall predicted probabilities by probe type
pred <- ggpredict(best_model, terms = c("dist_from_gt [all]", "choice", "timeLimit"))
p <- plot(pred)
p + labs(title = "Predicted Probabilities of Response 'j'",
color = "Probe Type", fill = "Probe Type",
x = "Distance from Ground Truth", y = "Predicted Probability of 'j'") +
scale_color_manual(values = c("lp" = "lightblue", "mid" = "royalblue")) +
scale_fill_manual(values = c("lp" = "lightblue", "mid" = "royalblue"))
# Subject-specific predictions
df_mid_lp$pred <- predict(best_model, type = "response")
ggplot(df_mid_lp, aes(x = dist_from_gt, y = pred, group = subject_id, color = subject_id)) +
geom_line(alpha = 0.6) +
facet_wrap(~ choice) +
labs(title = "Subject-specific Predicted Probabilities (Original Data)",
x = "Distance from Ground Truth", y = "Predicted Probability of 'j'") +
theme_minimal() +
theme(legend.position = "none")
# ============================================================================
# Section 6: Prediction Performance Evaluation
# ============================================================================
# Confusion Matrix & Accuracy
# Convert predicted probabilities to binary predictions using a threshold of 0.5
df_mid_lp$pred_class <- ifelse(df_mid_lp$pred > 0.5, 1, 0)
conf_mat <- confusionMatrix(factor(df_mid_lp$pred_class), factor(df_mid_lp$response_numeric))
print(conf_mat)
# Overall accuracy is about 0.567, which is modest.
# The model shows high sensitivity (0.86), meaning it correctly identifies most cases of the reject class (0),
# but specificity is low (0.31), so it struggles to correctly classify the acceptance class (1).
# ROC Curve & AUC
roc_obj <- roc(response = df_mid_lp$response_numeric, predictor = df_mid_lp$pred)
auc_value <- auc(roc_obj)
cat("AUC:", auc_value, "\n")  # AUC is approximately 0.67, indicating moderate discrimination
plot(roc_obj, main = "ROC Curve for Best Model Predictions")
# ============================================================================
# Section 7: Additional Analysis using Data Subset
# ============================================================================
# Subset the data to a shared range of dist_from_gt (e.g., < 130) for mid/lp probe types
# (since some lp probes are further away from the ground truth)
df_sub <- df_mid_lp[df_mid_lp$dist_from_gt < 130, ]
# Model 1: Full interaction: dist_from_gt + choice * timeLimit + random intercept per subject
model1_s <- gam(response_numeric ~ dist_from_gt + choice * timeLimit + s(subject_id, bs = "re"),
data = df_sub, family = binomial(link = "logit"))
# Model 2: Model 1 with a spline function on dist_from_gt
model2_s <- gam(response_numeric ~ s(dist_from_gt) + choice * timeLimit + s(subject_id, bs = "re"),
data = df_sub, family = binomial(link = "logit"))
# Model 3: Model 2 with a different smooth for dist_from_gt for each probe type (best model from previous section)
model3_s <- gam(response_numeric ~ s(dist_from_gt, by = choice) + choice * timeLimit + s(subject_id, bs = "re"),
data = df_sub, family = binomial(link = "logit"))
AIC(model1_s, model2_s, model3_s) # model3_s is the best model
BIC(model1_s, model2_s, model3_s)
models_s <- list(m1 = model1_s, m2 = model2_s, m3 = model3_s)
r2_values_s <- sapply(models_s, function(mod) summary(mod)$r.sq)
r2_values_s # model 3_s has the highest R^2
anova(model1_s, model2_s, model3_s)
# Model 3_s is the best model
best_model_s <- gam(response_numeric ~ s(dist_from_gt, by = choice) + choice*timeLimit + s(subject_id, bs = "re"),
data = df_sub, family = binomial(link = "logit"), method = "REML")
summary(best_model_s)
appraise(best_model_s)                                         ## fair
set.seed(42)
vgam_best_s <- getViz(best_model_s, nsim = 100, post = TRUE, unconditional = TRUE)   ## simulate 100 response vectors
check0D(vgam_best_s, type = "y") + l_dens1D()                 ## some discrepancies
# Generate predictions for the subset
pred_s <- ggpredict(best_model_s, terms = c("dist_from_gt [all]", "choice", "timeLimit"))
p_s <- plot(pred_s)
p_s + labs(title = "Predicted Probabilities of Response 'j'",
color = "Probe Type", fill = "Probe Type",
x = "Distance from Ground Truth", y = "Predicted Probability of 'j'") +
scale_color_manual(values = c("lp" = "lightblue", "mid" = "royalblue")) +
scale_fill_manual(values = c("lp" = "lightblue", "mid" = "royalblue"))
# greater distances are associated with a lower probability of a “j” response (not very clear from the spline)
# under the 3000ms condition, the baseline (for “lp”) has lower probability of a “j” response compared to the 700ms condition.
# when participants have 3000ms, the difference in how often they respond “j” between the “mid” and “lp” conditions is larger than when they have only 700ms.
# Evaluate prediction performance for the subset
df_sub$pred_class <- ifelse(df_sub$pred > 0.5, 1, 0)
conf_mat <- confusionMatrix(factor(df_sub$pred_class), factor(df_sub$response_numeric))
print(conf_mat)  # Accuracy is about 0.63, with relatively moderate specificity (0.71)
# ============================================================================
# Section 1: Data Preparation & Cleaning
# ============================================================================
# Load the dataset
data_full <- read_csv("df_all_pilot_v2_19participants.csv")
cat("Dimensions of full dataset: ", dim(data_full), "\n")
head(data_full)
# ============================================================================
# Section 0: Setup & Package Loading
# ============================================================================
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
require(pacman)
p_load(visdat, vtree, tidyverse, lme4, lmerTest, glmmTMB, performance,
GGally, datasets, sjPlot, mgcv, ggstatsplot, caret, pROC, ggeffects, ggplot2, mgcViz, gratia, itsadug, gamm4)
# ============================================================================
# Section 1: Data Preparation & Cleaning
# ============================================================================
# Load the dataset
data_full <- read_csv("df_all_pilot_v2_19participants.csv")
cat("Dimensions of full dataset: ", dim(data_full), "\n")
head(data_full)
str(data_full)
# Recode responses and compute derived distance variable
data_full$response_numeric <- ifelse(data_full$response == "j", 1, 0) # recode j/f responses to 1/0
data_full$dist_from_gt <- sqrt(
(data_full$obstacle_choice_x - data_full$obstacle_groundTruth_x)^2 +
(data_full$obstacle_choice_y - data_full$obstacle_groundTruth_y)^2
)
# Check if there are any missing values
vis_miss(data_full) # missing data in response & rt
# Only keep the complete cases in the analysis
data_full <- data_full[!is.na(data_full$response), ]
# Subset rows where choice is either "mid" or "lp" (will only be focusing on response to these two probe types)
df_mid_lp <- data_full[data_full$choice %in% c("mid", "lp"), ]
# Check the levels of categorical variables
df_mid_lp$choice <- factor(df_mid_lp$choice, levels = c("lp", "mid")) # Convert 'choice' to a factor with levels ordered as "lp" then "mid"
df_mid_lp$timeLimit <- factor(df_mid_lp$timeLimit)
df_mid_lp$subject_id <- factor(df_mid_lp$subject_id)
# Check participant counts and variable tree structure
table(df_mid_lp$subject_id) # max 96 measurements per participant
n_distinct(df_mid_lp$subject_id) # 19 participants
vtree(df_mid_lp, vars = ~ stimulus_idx + choice + timeLimit)
# ============================================================================
# Section 2: Exploratory Data Analysis (EDA)
# ============================================================================
vis_miss(df_mid_lp)  # No missing data now
# histogram
hist(df_mid_lp$dist_from_gt, main = "Dist from GT", xlab = "Dist from GT")
# Bar plot for response (numeric coded)
ggplot(df_mid_lp, aes(x = factor(response_numeric))) +
geom_bar() +
labs(title = "Frequency of Response (1 = Accept)", x = "Response", y = "Count")
# Scatterplot: Distance vs. Response
ggplot(df_mid_lp, aes(x = dist_from_gt, y = response_numeric, color = choice)) +
geom_jitter(width = 0.5, height = 0.1, alpha = 0.6) +
labs(title = "Scatterplot: Distance vs. Response",
x = "Distance from Ground Truth", y = "Response (1 = Accept)", color = "Choice",
fill = "Choice")
# Aggregated proportion of acceptance across participants (by condition)
df_summary <- df_mid_lp %>%
group_by(subject_id, choice, timeLimit) %>%
summarise(prop_j = mean(response_numeric), .groups = "drop")
ggplot(df_summary, aes(x = choice, y = prop_j, fill = timeLimit)) +
geom_boxplot(position = position_dodge(width = 0.8), alpha = 0.7, outlier.shape = NA) +
geom_jitter(position = position_jitterdodge(dodge.width = 0.8, jitter.width = 0.2), alpha = 0.5) +
labs(title = "Proportion of 'j' Responses by Probe Type and Time Limit",
x = "Choice", y = "Proportion of 'j' Responses", fill = "Time Limit (ms)") +
theme_minimal(base_size = 14)
# ============================================================================
# Section 3: Model Fitting - Candidate GAMs
# ============================================================================
# All models include a random intercept for subject.
# Model 1: Full interaction (parametric): dist_from_gt + choice * timeLimit + random intercept per subject
model1 <- gam(response_numeric ~ dist_from_gt + choice * timeLimit + s(subject_id, bs = "re"),
data = df_mid_lp, family = binomial(link = "logit"))
summary(model1)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
require(pacman)
p_load(tidyverse, lme4, lmerTest, glmmTMB, performance, GGally, datasets, sjPlot, mgcv, ggstatsplot)
data_full <- read_csv("../sanityCheck_rawData/df_all_pilot_v2_19participants.csv")
dim(data_full)
head(data_full)
data_full$response_numeric <- ifelse(data_full$response == "j", 1, 0)
data_full <- data_full[!is.na(data_full$response), ]
data_full$dist_from_gt <- sqrt(
(data_full$obstacle_choice_x - data_full$obstacle_groundTruth_x)^2 +
(data_full$obstacle_choice_y - data_full$obstacle_groundTruth_y)^2
)
# Subset rows where choice is either "mid" or "lp"
df_mid_lp <- data_full[data_full$choice %in% c("mid", "lp"), ]
# Convert 'choice' to a factor with levels ordered as "lp" then "mid"
df_mid_lp$choice <- factor(df_mid_lp$choice, levels = c("lp", "mid"))
df_mid_lp$timeLimit <- factor(df_mid_lp$timeLimit)
df_mid_lp$subject_id <- factor(df_mid_lp$subject_id)
# Model 1: Full interaction: dist_from_gt + choice * timeLimit + random intercept per subject
model1 <- glmer(response_numeric ~ dist_from_gt + choice * timeLimit + (1 | subject_id),
data = df_mid_lp, family = binomial(link = "logit"))
# Model 2: Main effects: dist_from_gt + choice + timeLimit + random intercept per subject
model2 <- glmer(response_numeric ~ dist_from_gt + choice + timeLimit + (1 | subject_id),
data = df_mid_lp, family = binomial(link = "logit"))
# Model 3: No choice effect: dist_from_gt + timeLimit + random intercept per subject
model3 <- glmer(response_numeric ~ dist_from_gt + timeLimit + (1 | subject_id),
data = df_mid_lp, family = binomial(link = "logit"))
# Model 4: No timeLimit effect: dist_from_gt + choice + random intercept per subject
model4 <- glmer(response_numeric ~ dist_from_gt + choice + (1 | subject_id),
data = df_mid_lp, family = binomial(link = "logit"))
# Model 5: Distance only: dist_from_gt + random intercept per subject
model5 <- glmer(response_numeric ~ dist_from_gt + (1 | subject_id),
data = df_mid_lp, family = binomial(link = "logit"))
# Model 6: model4 with a spline function
model6 <- gam(response_numeric ~ s(dist_from_gt) + choice + s(subject_id, bs = "re"),
data = df_mid_lp, family = binomial(link = "logit"))
AIC(model1, model2, model3, model4, model5 , model6)
BIC(model1, model2, model3, model4, model5, model6)
anova(model1, model2, model4)
anova(model1, model4)
anova(model1, model2)
anova(model1, model2, model3, model5)
anova(model6, model4)
anova(model1, model2, model4, model5)
# Model 1 is the best model
summary(model1)
library(ggeffects)
library(ggplot2)
plot_model(model1, type = "pred", terms = c("dist_from_gt", "choice", "timeLimit"))
# Generate predictions over a range of distances, separately for each 'choice'
# Here, "dist_from_gt" will be varied and "choice" held at its levels.
pred <- ggpredict(model1, terms = c("dist_from_gt [all]", "choice", "timeLimit"))
p <- plot(pred)
# Customize the plot: change title, legend labels, and colors for 'choice'
p +
labs(title = "Predicted probabilities of Response_j",
color = "Probe Type",
fill = "Probe Type",
x = "Distance from Ground Truth",
y = "Predicted Probability of 'j'") +
scale_color_manual(values = c("lp" = "lightblue", "mid" = "royalblue")) +
scale_fill_manual(values = c("lp" = "lightblue", "mid" = "royalblue"))
# Plot the predicted probabilities - v1
ggplot(pred, aes(x = x, y = predicted, color = group)) +
geom_line(size = 1.2) +
geom_ribbon(aes(ymin = conf.low, ymax = conf.high, fill = group),
alpha = 0.2, color = NA) +
labs(x = "Distance from Ground Truth",
y = "Predicted Probability of 'j'",
color = "Choice",
fill = "Choice") +
theme_minimal()
df_mid_lp$pred <- predict(model1)
ggplot(df_mid_lp, aes(x = dist_from_gt, y = pred, group = subject_id, color = subject_id)) +
geom_line(alpha = 0.6) +
facet_wrap(~ choice) +
labs(x = "Distance from Ground Truth",
y = "Predicted Probability of 'j'",
title = "Subject-specific Predicted Probabilities (Original Data)") +
theme_minimal() +
theme(legend.position = "none")
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
require(pacman)
p_load(tidyverse, lme4, lmerTest, glmmTMB, performance, GGally, datasets, sjPlot, mgcv, ggstatsplot)
data_full <- read_csv("../sanityCheck_rawData/df_all_pilot_v2_19participants.csv")
dim(data_full)
head(data_full)
data_full$response_numeric <- ifelse(data_full$response == "j", 1, 0)
data_full <- data_full[!is.na(data_full$response), ]
data_full$dist_from_gt <- sqrt(
(data_full$obstacle_choice_x - data_full$obstacle_groundTruth_x)^2 +
(data_full$obstacle_choice_y - data_full$obstacle_groundTruth_y)^2
)
# Subset rows where choice is either "mid" or "lp"
df_mid_lp <- data_full[data_full$choice %in% c("gt", "mid", "lp"), ]
# Convert 'choice' to a factor with levels ordered as "lp" then "mid"
df_mid_lp$choice <- factor(df_mid_lp$choice, levels = c("gt", "lp", "mid"))
df_mid_lp$timeLimit <- factor(df_mid_lp$timeLimit)
df_mid_lp$subject_id <- factor(df_mid_lp$subject_id)
# Model 1: Full interaction: dist_from_gt + choice * timeLimit + random intercept per subject
model1 <- glmer(response_numeric ~ dist_from_gt + choice * timeLimit + (1 | subject_id),
data = df_mid_lp, family = binomial(link = "logit"))
summary(model1)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
require(pacman)
p_load(tidyverse, lme4, lmerTest, glmmTMB, performance, GGally, datasets, sjPlot, mgcv, ggstatsplot)
data_full <- read_csv("../sanityCheck_rawData/df_all_pilot_v2_19participants.csv")
dim(data_full)
head(data_full)
data_full$response_numeric <- ifelse(data_full$response == "j", 1, 0)
data_full <- data_full[!is.na(data_full$response), ]
data_full$dist_from_gt <- sqrt(
(data_full$obstacle_choice_x - data_full$obstacle_groundTruth_x)^2 +
(data_full$obstacle_choice_y - data_full$obstacle_groundTruth_y)^2
)
# Subset rows where choice is either "mid" or "lp"
df_mid_lp <- data_full[data_full$choice %in% c("gt", "mid", "lp"), ]
# Convert 'choice' to a factor with levels ordered as "lp" then "mid"
df_mid_lp$choice <- factor(df_mid_lp$choice, levels = c("lp", "gt", "mid"))
df_mid_lp$timeLimit <- factor(df_mid_lp$timeLimit)
df_mid_lp$subject_id <- factor(df_mid_lp$subject_id)
# Model 1: Full interaction: dist_from_gt + choice * timeLimit + random intercept per subject
model1 <- glmer(response_numeric ~ dist_from_gt + choice * timeLimit + (1 | subject_id),
data = df_mid_lp, family = binomial(link = "logit"))
summary(model1)
